# ü§ñ AgentQA

Sistema de Perguntas e Respostas (QA) com IA utilizando **RAG (Retrieval-Augmented Generation)** local, implementado como uma API FastAPI.

## üìÇ Estrutura de Pastas

- `src/core/`: Configura√ß√µes globais e valida√ß√£o de ambiente com **Pydantic Settings**.
- `src/infra/`: Gerenciamento de persist√™ncia (LanceDB) e conhecimento.
- `src/services/`: L√≥gica de constru√ß√£o e orquestra√ß√£o dos Agentes.
- `src/api/`: Endpoints da API FastAPI.

## üõ†Ô∏è Instala√ß√£o e Configura√ß√£o

### Pr√©-requisitos
- [uv](https://github.com/astral-sh/uv) para gerenciamento de pacotes.
- **Para execu√ß√£o local**: [Ollama](https://ollama.ai/) rodando com os modelos:
  - `llama3.1` (LLM)
  - `nomic-embed-text` (Embeddings)
- **Para execu√ß√£o com Docker** (opcional): [Docker](https://www.docker.com/) e [Docker Compose](https://docs.docker.com/compose/).

Verifique se o Ollama est√° rodando e tem os modelos (para execu√ß√£o local):
```bash
ollama list
```
Se n√£o tiver os modelos, baixe-os:
```bash
ollama pull llama3.1
ollama pull nomic-embed-text
```

### Depend√™ncias
Instale as depend√™ncias do projeto:
```bash
uv sync
```

### Configura√ß√£o (.env)
Crie um arquivo `.env` na raiz do projeto com as seguintes vari√°veis:
```env
LLM_MODEL=llama3.1
EMBEDDER_MODEL=nomic-embed-text
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
EMBEDDER_HOST=localhost
EMBEDDER_PORT=11434  # Mesmo host/porta para ambos, pois Ollama local serve m√∫ltiplos modelos
ENABLE_WEB_SEARCH=true
```

## üöÄ Como Executar

### Op√ß√£o 1: Execu√ß√£o Local (Recomendado para Desenvolvimento)
Para iniciar o servidor FastAPI:
```bash
make run
```
Ou diretamente:
```bash
uv run uvicorn src.main:app --reload --host 0.0.0.0 --port 8000
```

Para carregar os dados de exemplo (seed) antes de iniciar:
```bash
make seed
```

### Op√ß√£o 2: Execu√ß√£o Local com Ollama em Docker (Para quem n√£o tem Ollama instalado)
Use os cont√™ineres Docker para os modelos Ollama, mas execute a aplica√ß√£o localmente.

1. **Inicie os servi√ßos Ollama em background**:
   ```bash
   docker compose up -d llama-service embed-service
   ```
   Isso exp√µe o LLM na porta 11434 e o embedder na porta 11435.

2. **Configure o .env para Docker**:
   ```env
   OLLAMA_HOST=localhost
   OLLAMA_PORT=11434
   EMBEDDER_HOST=localhost
   EMBEDDER_PORT=11435
   ```

3. **Execute a aplica√ß√£o**:
   ```bash
   make seed && make run
   ```

**Para parar os cont√™ineres**: `docker compose down`.

### Op√ß√£o 3: Execu√ß√£o com Docker (Para Produ√ß√£o ou Isolamento)
O projeto inclui configura√ß√µes Docker para rodar os modelos Ollama em cont√™ineres isolados.

#### Pr√©-requisitos para Docker
- [Docker](https://www.docker.com/) e [Docker Compose](https://docs.docker.com/compose/) instalados.

#### Passos para Executar com Docker
1. **Construa e inicie os servi√ßos Ollama**:
   ```bash
   docker compose up --build
   ```
   Isso criar√° dois cont√™ineres:
   - `llama-service`: Modelo LLM (`llama3.1`) na porta 11434.
   - `embed-service`: Modelo de embeddings (`nomic-embed-text`) na porta 11435.

2. **Configure o .env para Docker**:
   Edite o .env para apontar para os nomes dos servi√ßos na rede Docker:
   ```env
   OLLAMA_HOST=llama-service
   OLLAMA_PORT=11434
   EMBEDDER_HOST=embed-service
   EMBEDDER_PORT=11434  # Porta interna dos cont√™ineres
   ```

3. **Execute a aplica√ß√£o**:
   Com os cont√™ineres rodando em background, execute a aplica√ß√£o localmente:
   ```bash
   make seed && make run
   ```

**Nota**: Os modelos s√£o baixados durante a constru√ß√£o das imagens, o que pode levar tempo na primeira execu√ß√£o. Para parar os cont√™ineres: `docker compose down`.

### Sess√£o de Testes
Use os comandos abaixo para executar os testes do projeto:

```bash
# Rodar todos os testes
uv run pytest -v

# Rodar com cobertura
uv run pytest --cov=src --cov-report=term-missing

# Rodar s√≥ um m√≥dulo
uv run pytest tests/api/test_ask.py -v
```

## üì° API Endpoints

A API est√° dispon√≠vel em `http://localhost:8000` (ou conforme configurado).

- **GET /**: Health check da API.
- **POST /ask**: Faz uma pergunta ao agente. Corpo: `{"question": "Sua pergunta aqui"}`.
- **POST /questions/generate**: Cria quest√µes baseado na base de conhecimento fornecida. Corpo: `{"topic": "Topico da Pergunta", "num_questions": 5, "difficulty": "Nivel de dificuldade"}`
- **POST /docs/add**: Adiciona um documento √† base de conhecimento. Corpo: `{"text": "Conte√∫do do documento"}`.

Use ferramentas como Postman, curl ou a documenta√ß√£o autom√°tica do FastAPI em docs para testar.

## Detalhes da Implementa√ß√£o

### Escolha da Base Vetorial
Poder√≠amos usar o PgVector (PostgreSQL) ou ChromaDB, mas o LanceDB √© super leve e f√°cil de usar localmente, sem precisar de um servidor. Conseguimos usar o OllamaEmbedder para criar os vetores de embedding usando um modelo local da Ollama, como o `nomic-embed-text`, que √© gratuito e open-source. O LanceDB √© √≥timo para prototipagem r√°pida e pequenos projetos, e tem uma API bem simples para inserir e buscar dados. Ele armazena os vetores de embedding junto com o conte√∫do original, o que facilita a recupera√ß√£o de informa√ß√µes relevantes durante as conversas do agente. Al√©m disso, o LanceDB √© super r√°pido e eficiente, mesmo com um grande n√∫mero de documentos, gra√ßas √† sua estrutura otimizada para buscas vetoriais.

### Configura√ß√£o dos Embeddings
O modelo `nomic-embed-text` gera embeddings de 768 dimens√µes, mas o LanceDB pode esperar 4096 por padr√£o. Por isso, definimos explicitamente `dimensions=768` no `OllamaEmbedder` para garantir compatibilidade.

### Modelo de Linguagem
Usamos o `llama3.1` como LLM local via Ollama, pois n√£o conseguimos usar ferramentas (tools) com modelos pequenos (SMLs) como o `phi3`. O `llama3.1` √© √≥timo para RAG e tem um custo super baixo rodando localmente.

### Ferramentas e Mem√≥ria
- As ferramentas como DuckDuckGo s√£o opcionais para buscas na web.
- O hist√≥rico de conversas √© armazenado em SQLite para mem√≥ria persistente, com at√© 3 execu√ß√µes anteriores inclu√≠das no contexto.

### Dados de Exemplo
Os dados inseridos na base de conhecimento s√£o exemplos variados (f√°ceis, m√©dios e dif√≠ceis) para testar o RAG.

## Problemas Conhecidos

### Uso de Modelos Pequenos (SMLs) com Tools

Alguns modelos como o `phi3` n√£o suportam o uso de ferramentas (tools) no Agno, conforme a documenta√ß√£o. Para usar modelos menores localmente, considere alternativas como:

- **phi4** ou **qwen2.5-7b**: Modelos pequenos mas poderosos.
- **Provedores Locais**: Ollama, LM Studio, LlamaCpp ou VLLM.

Exemplo de uso com LM Studio:

```python
from agno.agent import Agent
from agno.models.lmstudio import LMStudio

agent = Agent(
    model=LMStudio(id="qwen2.5-7b-instruct-1m"),
    markdown=True,
)
agent.print_response("Ol√°!")
```

Certifique-se de que o modelo suporte tools se precisar dessa funcionalidade.


## Refer√™ncias da Documenta√ß√£o do Agno

- [Ferramentas de Busca - DuckDuckGo](https://docs.agno.com/tools/toolkits/search/duckduckgo)
- [Ferramentas de Busca - Brave Search](https://docs.agno.com/tools/toolkits/search/bravesearch)
- [LanceDB - Vector Databases](https://docs.agno.com/cookbook/knowledge/vector-databases)
- [LanceDB Overview](https://docs.agno.com/knowledge/vector-stores/lancedb/overview)
- [Hist√≥rico de Chat](https://docs.agno.com/database/chat-history)
- [Gerenciamento de Sess√µes](https://docs.agno.com/sessions/history-management)
- [Hist√≥rico do Agente](https://docs.agno.com/history/agent/overview)
- [Modelos Locais com Ollama](https://docs.agno.com/models/ollama)
- [Modelos com LM Studio](https://docs.agno.com/models/lmstudio)
- [KnowledgeTools](https://docs.agno.com/tools/toolkits/others/knowledge)
- [Reasoning](https://docs.agno.com/reasoning/reasoning-tools)
- [Engenharia de Contexto](https://docs.agno.com/context/agent/overview)
- [Traga seu pr√≥prio aplicativo FastAPI](https://docs.agno.com/agent-os/custom-fastapi/overview)
- [O que √© o AgentOS](https://docs.agno.com/agent-os/introduction)
- [Execute seu AgentOS](https://docs.agno.com/agent-os/run-your-os)


Para mais informa√ß√µes, visite a [documenta√ß√£o oficial do Agno](https://docs.agno.com/).

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.
