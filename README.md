# ü§ñ AgentQA

Sistema de Perguntas e Respostas (QA) com IA utilizando **RAG (Retrieval-Augmented Generation)** local.

## üìÇ Estrutura de Pastas

- `src/core/`: Configura√ß√µes globais e valida√ß√£o de ambiente com **Pydantic Settings**.
- `src/infra/`: Gerenciamento de persist√™ncia (LanceDB) e conhecimento.
- `src/services/`: L√≥gica de constru√ß√£o e orquestra√ß√£o do Agente.
- `src/ui/`: Interface de usu√°rio (CLI interativa).

## üõ†Ô∏è Instala√ß√£o e Configura√ß√£o

### Pr√©-requisitos
- [uv](https://github.com/astral-sh/uv) para gerenciamento de pacotes.
- [Ollama](https://ollama.ai/) rodando com os modelos:
  - `llama3.1` (LLM)
  - `nomic-embed-text` (Embeddings)

Verifique se o Ollama est√° rodando e tem os modelos:
```bash
ollama list
```
Se n√£o tiver os modelos, baixe-os:
```bash
ollama pull llama3.1
ollama pull nomic-embed-text
```

### Depend√™ncias
Instale as depend√™ncias do projeto:
```bash
uv sync
```

### Configura√ß√£o (.env)
Crie um arquivo `.env` na raiz do projeto com as seguintes vari√°veis:
```env
LLM_MODEL=llama3.1
EMBEDDER_MODEL=nomic-embed-text
ENABLE_WEB_SEARCH=true
```

## üöÄ Como Executar

Para iniciar a CLI interativa:
```bash
uv run python -m src.main
```

Para carregar os dados de exemplo (seed) e iniciar:
```bash
uv run python -m src.main --seed
```

Para uma pergunta direta via terminal:
```bash
uv run python -m src.main --ask "Qual a capital da Fran√ßa?"
```

## üìù Comandos na CLI

- `/add`: Adiciona novos textos √† base de conhecimento em tempo real.
- `/help`: Mostra a lista de comandos.
- `/quit`: Encerra o programa.

## Detalhes da Implementa√ß√£o

### Escolha da Base Vetorial
Poder√≠amos usar o PgVector (PostgreSQL) ou ChromaDB, mas o LanceDB √© super leve e f√°cil de usar localmente, sem precisar de um servidor. Conseguimos usar o OllamaEmbedder para criar os vetores de embedding usando um modelo local da Ollama, como o `nomic-embed-text`, que √© gratuito e open-source. O LanceDB √© √≥timo para prototipagem r√°pida e pequenos projetos, e tem uma API bem simples para inserir e buscar dados. Ele armazena os vetores de embedding junto com o conte√∫do original, o que facilita a recupera√ß√£o de informa√ß√µes relevantes durante as conversas do agente. Al√©m disso, o LanceDB √© super r√°pido e eficiente, mesmo com um grande n√∫mero de documentos, gra√ßas √† sua estrutura otimizada para buscas vetoriais.

### Configura√ß√£o dos Embeddings
O modelo `nomic-embed-text` gera embeddings de 768 dimens√µes, mas o LanceDB pode esperar 4096 por padr√£o. Por isso, definimos explicitamente `dimensions=768` no `OllamaEmbedder` para garantir compatibilidade.

### Modelo de Linguagem
Usamos o `llama3.1` como LLM local via Ollama, pois n√£o conseguimos usar ferramentas (tools) com modelos pequenos (SMLs) como o `phi3`. O `llama3.1` √© √≥timo para RAG e tem um custo super baixo rodando localmente.

### Ferramentas e Mem√≥ria
- As ferramentas como DuckDuckGo s√£o opcionais para buscas na web.
- O hist√≥rico de conversas √© armazenado em SQLite para mem√≥ria persistente, com at√© 3 execu√ß√µes anteriores inclu√≠das no contexto.

### Dados de Exemplo
Os dados inseridos na base de conhecimento s√£o exemplos variados (f√°ceis, m√©dios e dif√≠ceis) para testar o RAG.

## Problemas Conhecidos

### Uso de Modelos Pequenos (SMLs) com Tools

Algums modelos como o `phi3` n√£o suporta o uso de ferramentas (tools) no Agno, conforme a documenta√ß√£o. Para usar modelos menores localmente, considere alternativas como:

- **phi4** ou **qwen2.5-7b**: Modelos pequenos mas poderosos.
- **Provedores Locais**: Ollama, LM Studio, LlamaCpp ou VLLM.

Exemplo de uso com LM Studio:

```python
from agno.agent import Agent
from agno.models.lmstudio import LMStudio

agent = Agent(
    model=LMStudio(id="qwen2.5-7b-instruct-1m"),
    markdown=True,
)
agent.print_response("Ol√°!")
```

Certifique-se de que o modelo suporte tools se precisar dessa funcionalidade.

### Instala√ß√£o obrigat√≥ria de pacotes/libs

Embora o projeto utilize modelos locais via Ollama (evitando depend√™ncias de APIs externas como OpenAI), o Agno requer algumas bibliotecas como depend√™ncias obrigat√≥rias. Mesmo que n√£o sejam usadas diretamente no c√≥digo, elas s√£o necess√°rias para o funcionamento do framework:

- `"openai"`: Biblioteca para integra√ß√£o com OpenAI, mas n√£o utilizada aqui, pois optamos por modelos locais.
- `"sqlalchemy"`: ORM para bancos de dados, usado internamente pelo Agno para gerenciar o hist√≥rico e outras funcionalidades.

Essas depend√™ncias s√£o instaladas automaticamente ao executar `uv sync`. Se preferir instalar manualmente:

```bash
uv add openai sqlalchemy
```

## Refer√™ncias da Documenta√ß√£o do Agno

- [Ferramentas de Busca - DuckDuckGo](https://docs.agno.com/tools/toolkits/search/duckduckgo)
- [Ferramentas de Busca - Brave Search](https://docs.agno.com/tools/toolkits/search/bravesearch)
- [LanceDB - Vector Databases](https://docs.agno.com/cookbook/knowledge/vector-databases)
- [LanceDB Overview](https://docs.agno.com/knowledge/vector-stores/lancedb/overview)
- [Hist√≥rico de Chat](https://docs.agno.com/database/chat-history)
- [Gerenciamento de Sess√µes](https://docs.agno.com/sessions/history-management)
- [Hist√≥rico do Agente](https://docs.agno.com/history/agent/overview)
- [Modelos Locais com Ollama](https://docs.agno.com/models/ollama)
- [Modelos com LM Studio](https://docs.agno.com/models/lmstudio)

Para mais informa√ß√µes, visite a [documenta√ß√£o oficial do Agno](https://docs.agno.com/).

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.
